from mcp.server.fastmcp import FastMCP
import sqlite3
import requests
from bs4 import BeautifulSoup
import json
import os
import re
from urllib.parse import urljoin

# ë„ì»¤ ë³¼ë¥¨ì— ì €ìž¥ë  DB ê²½ë¡œ
DB_PATH = "/data/config.db"
mcp = FastMCP("OmniAnalyst")

def init_db():
    os.makedirs("/data", exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    # í™˜ê²½ í…Œì´ë¸”
    conn.execute('''
        CREATE TABLE IF NOT EXISTS environments (
            name TEXT PRIMARY KEY, 
            description TEXT
        )
    ''')
    # ì‚¬ì´íŠ¸ ì„¤ì • í…Œì´ë¸”
    conn.execute('''
        CREATE TABLE IF NOT EXISTS sites (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            env_name TEXT,
            site_name TEXT,
            board_url TEXT,
            title_selector TEXT,
            comment_selector TEXT,
            link_selector TEXT,
            FOREIGN KEY(env_name) REFERENCES environments(name)
        )
    ''')
    conn.commit()
    conn.close()

# ì„œë²„ ì‹œìž‘ ì‹œ DB ì´ˆê¸°í™”
init_db()

# --- âš™ï¸ ì„¤ì • ê´€ë¦¬ ë„êµ¬ ---

@mcp.tool()
def create_environment(name: str, description: str = "") -> str:
    """ìƒˆë¡œìš´ ê°ì‹œ í™˜ê²½ì„ ë§Œë“­ë‹ˆë‹¤. (ì˜ˆ: í•«ë”œ, ë‰´ìŠ¤)"""
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute("INSERT INTO environments VALUES (?, ?)", (name, description))
        conn.commit()
        return f"âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ: {name}"
    except sqlite3.IntegrityError:
        return "âš ï¸ ì´ë¯¸ ì¡´ìž¬í•˜ëŠ” í™˜ê²½ìž…ë‹ˆë‹¤."
    finally:
        conn.close()

@mcp.tool()
def add_board_to_env(env_name: str, site_name: str, board_url: str, title_selector: str, comment_selector: str, link_selector: str = "") -> str:
    """í™˜ê²½ì— ê²Œì‹œíŒ ì‚¬ì´íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. link_selectorëŠ” ë¹„ì›Œë‘ë©´ ìžë™ ì¶”ë¡ í•©ë‹ˆë‹¤."""
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute(
            "INSERT INTO sites (env_name, site_name, board_url, title_selector, comment_selector, link_selector) VALUES (?, ?, ?, ?, ?, ?)",
            (env_name, site_name, board_url, title_selector, comment_selector, link_selector)
        )
        conn.commit()
        return f"âœ… ì‚¬ì´íŠ¸ ì¶”ê°€ ì™„ë£Œ: {site_name}"
    except Exception as e:
        return f"âŒ ì¶”ê°€ ì‹¤íŒ¨: {e}"
    finally:
        conn.close()

# --- ðŸ” ë°ì´í„° ìˆ˜ì§‘ ë„êµ¬ ---

@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """í•´ë‹¹ í™˜ê²½ì˜ ëª¨ë“  ê²Œì‹œíŒ 1íŽ˜ì´ì§€ë¥¼ ê¸ì–´ì™€ [ì œëª©, ëŒ“ê¸€ìˆ˜, ë§í¬] ë¦¬ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    conn = sqlite3.connect(DB_PATH)
    sites = conn.execute("SELECT site_name, board_url, title_selector, comment_selector, link_selector FROM sites WHERE env_name = ?", (env_name,)).fetchall()
    conn.close()

    if not sites:
        return json.dumps({"error": f"'{env_name}' í™˜ê²½ì— ë“±ë¡ëœ ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."})

    all_items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

    for site_name, url, t_sel, c_sel, l_sel in sites:
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ì œëª© ìš”ì†Œë“¤ ì°¾ê¸°
            titles = soup.select(t_sel)
            
            # ìƒìœ„ 15ê°œë§Œ ì²˜ë¦¬
            for t_el in titles[:15]:
                item = {
                    "site": site_name, 
                    "title": t_el.get_text(strip=True), 
                    "comments": 0, 
                    "link": ""
                }
                
                # 1. ë§í¬ ì¶”ì¶œ
                a_tag = None
                if l_sel:
                    a_tag = t_el if t_el.name == 'a' else t_el.select_one(l_sel)
                
                # ë§í¬ ì„ íƒìžê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•˜ë©´ ë¶€ëª¨/ìžì‹ íƒìƒ‰
                if not a_tag:
                    a_tag = t_el if t_el.name == 'a' else t_el.find_parent('a')
                
                if a_tag and a_tag.has_attr('href'):
                    item["link"] = urljoin(url, a_tag['href'])

                # 2. ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
                if c_sel:
                    c_text = ""
                    # A. ì œëª© íƒœê·¸ ë‚´ë¶€ì—ì„œ ì°¾ê¸°
                    c_tag = t_el.select_one(c_sel)
                    if c_tag:
                        c_text = c_tag.get_text()
                    # B. ì—†ìœ¼ë©´ ë¶€ëª¨ì˜ ìžì‹(í˜•ì œ) ì¤‘ì—ì„œ ì°¾ê¸°
                    elif t_el.parent:
                        c_tag_parent = t_el.parent.select_one(c_sel)
                        if c_tag_parent:
                            c_text = c_tag_parent.get_text()
                    
                    # ìˆ«ìžë§Œ ì¶”ì¶œ (ì˜ˆ: "[15]" -> 15)
                    nums = re.findall(r'\d+', c_text)
                    if nums:
                        item["comments"] = int(nums[0])

                all_items.append(item)

        except Exception as e:
            print(f"Error fetching {site_name}: {e}")
            continue

    return json.dumps(all_items, ensure_ascii=False)

if __name__ == "__main__":
    mcp.run()