from mcp.server.fastmcp import FastMCP
import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
from datetime import datetime

# ì•Œêµ¬ëª¬ ì£¼ì†Œ
ALGUMON_URL = "https://algumon.com"
mcp = FastMCP("OmniAnalyst")

# ğŸ”‡ ë¡œê·¸ í•¨ìˆ˜ ì‚­ì œ: ì´ì œ ì„œë²„ëŠ” ì•„ë¬´ëŸ° ì¶œë ¥ë„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì—ëŸ¬ ë°©ì§€)

@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ì•Œêµ¬ëª¬ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
    # log("ìŠ¤ìº” ì‹œì‘") -> ì‚­ì œë¨
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        resp = requests.get(ALGUMON_URL, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_items = []
        today_str = datetime.now().strftime("%m/%d")
        
        products = soup.select(".product-body")
        
        for post in products[:30]: 
            try:
                item = {
                    "site": "ì•Œêµ¬ëª¬",
                    "title": "",
                    "comments": 0,
                    "link": "",
                    "date_text": "",
                    "content_selector": "AUTO"
                }

                title_tag = post.select_one(".deal-title .item-name a")
                if title_tag:
                    item["title"] = title_tag.get_text(strip=True)
                    item["link"] = urljoin(ALGUMON_URL, title_tag.get('href'))
                else: continue

                comment_icon = post.select_one(".icon-commenting-o")
                if comment_icon:
                    cmt_text = comment_icon.parent.get_text(strip=True)
                    nums = re.findall(r'\d+', cmt_text)
                    if nums: item["comments"] = int(nums[0])

                raw_text = ""
                date_tag = post.select_one(".created-at")
                if date_tag: raw_text = date_tag.get_text(strip=True)
                else:
                    meta_tag = post.select_one(".deal-price-meta-info")
                    if meta_tag: raw_text = meta_tag.get_text(strip=True)

                clean_date = ""
                time_match = re.search(r'(\d+ë¶„\s*ì „|\d+ì‹œê°„\s*ì „|ë°©ê¸ˆ|\d+ì´ˆ\s*ì „|\d{2}-\d{2}|\d{2}/\d{2})', raw_text)
                if time_match: clean_date = time_match.group(1)
                else:
                    parts = raw_text.split()
                    if parts: clean_date = parts[-1]

                if any(x in clean_date for x in ["ë°©ê¸ˆ", "ë¶„", "ì‹œê°„", "ì´ˆ"]):
                    item["date_text"] = f"{today_str} ({clean_date})"
                else:
                    item["date_text"] = clean_date

                all_items.append(item)
            except: continue

        # log("í™•ë³´ ì™„ë£Œ") -> ì‚­ì œë¨
        return json.dumps(all_items, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"ì•Œêµ¬ëª¬ ì ‘ì† ì‹¤íŒ¨: {e}"}, ensure_ascii=False)


@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    """ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì  ë° ë³¸ë¬¸ ìˆ˜ì§‘"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://algumon.com/'
        }
        
        session = requests.Session()
        resp = session.get(url, headers=headers, timeout=10)
        
        # ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€
        if "ì´ë™ì¤‘" in resp.text or "redirect" in resp.url or "refresh" in resp.text.lower():
            # log("ëŒ€ê¸° í˜ì´ì§€ ê°ì§€") -> ì‚­ì œë¨
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            meta = soup.find("meta", attrs={"http-equiv": "refresh"})
            new_url = None
            
            if meta:
                content = meta.get("content", "")
                match = re.search(r"url=([^;'\"]+)", content, re.IGNORECASE)
                if match: new_url = match.group(1)
            
            if not new_url:
                match = re.search(r"location\.href\s*=\s*['\"]([^'\"]+)['\"]", resp.text)
                if match: new_url = match.group(1)
                
            if new_url:
                resp = session.get(new_url, headers=headers, timeout=10)

        final_url = resp.url
        # log(f"ìµœì¢… ì ‘ì†: {final_url}") -> ì‚­ì œë¨
        
        if "ppomppu.co.kr" in final_url:
            resp.encoding = 'cp949'
        else:
            resp.encoding = resp.apparent_encoding 
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ëŒ“ê¸€ ì°¾ê¸°
        extracted_text = []
        selectors = [
            ".han-comment", ".comment_wrapper", "#quote", ".list_comment", 
            ".comment-content", ".comment_view", ".xe_content", 
            ".reply", ".review", ".comment", ".list-group-item"
        ]
        
        for sel in selectors:
            found = soup.select(sel)
            if found:
                for el in found:
                    t = el.get_text(strip=True)
                    if t: extracted_text.append(f"- {t}")
        
        # ëŒ“ê¸€ ì—†ìœ¼ë©´ ë³¸ë¬¸
        if not extracted_text:
            # log("ëŒ“ê¸€ ì—†ìŒ, ë³¸ë¬¸ ìˆ˜ì§‘") -> ì‚­ì œë¨
            for s in soup(["script", "style", "iframe", "header", "footer", "nav"]):
                s.extract()
            full_text = soup.get_text(separator="\n", strip=True)
            full_text = re.sub(r'\n+', '\n', full_text)
            return f"[ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„]\n{full_text[:3500]}"

        return f"[ëŒ“ê¸€ ìˆ˜ì§‘ ì„±ê³µ]\n" + "\n".join(extracted_text[:50])

    except Exception as e:
        return f"ì ‘ì† ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()
