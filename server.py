from mcp.server.fastmcp import FastMCP
import sqlite3
import requests
from bs4 import BeautifulSoup
import json
import os
import re
from urllib.parse import urljoin
from datetime import datetime

# ì•Œêµ¬ëª¬ ì£¼ì†Œ
ALGUMON_URL = "https://algumon.com"
DB_PATH = "/data/config.db"
mcp = FastMCP("OmniAnalyst")

def init_db():
    os.makedirs("/data", exist_ok=True)
    # DB ê´€ë ¨ ì½”ë“œëŠ” ì—ëŸ¬ ë°©ì§€ìš©ìœ¼ë¡œ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
    conn = sqlite3.connect(DB_PATH)
    conn.execute('CREATE TABLE IF NOT EXISTS environments (name TEXT PRIMARY KEY, description TEXT)')
    conn.execute('CREATE TABLE IF NOT EXISTS sites (id INTEGER PRIMARY KEY, env_name TEXT, site_name TEXT)')
    conn.close()

init_db()

@mcp.tool()
def create_environment(name: str, description: str = "") -> str:
    return "âœ… ì•Œêµ¬ëª¬ ì „ìš© ëª¨ë“œì…ë‹ˆë‹¤."

@mcp.tool()
def add_board_to_env(env_name: str, site_name: str, board_url: str, title_selector: str, comment_selector: str, content_selector: str, date_selector: str, link_selector: str = "") -> str:
    return "âœ… ì•Œêµ¬ëª¬ ì „ìš© ëª¨ë“œë¼ ì„¤ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤."

@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ì•Œêµ¬ëª¬ ì „ìš© íŒŒì„œ (ë‚ ì§œ ìë™ ë³€í™˜ ê¸°ëŠ¥ í¬í•¨)"""
    print(f"ğŸ” [ì•Œêµ¬ëª¬] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        resp = requests.get(ALGUMON_URL, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_items = []
        today_str = datetime.now().strftime("%m/%d") # ì˜ˆ: "12/17"
        
        # .product-body í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  ìš”ì†Œë¥¼ ì°¾ìŒ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
        products = soup.select(".product-body")
        
        for post in products[:25]:
            try:
                item = {
                    "site": "ì•Œêµ¬ëª¬",
                    "title": "",
                    "comments": 0,
                    "link": "",
                    "date_text": "",
                    "content_selector": ".post-content"
                }

                # 1. ì œëª© & ë§í¬
                title_tag = post.select_one(".deal-title .item-name a")
                if title_tag:
                    item["title"] = title_tag.get_text(strip=True)
                    item["link"] = urljoin(ALGUMON_URL, title_tag.get('href'))
                else:
                    continue

                # 2. ëŒ“ê¸€ ìˆ˜ (ì•„ì´ì½˜ ì˜† ìˆ«ì ì°¾ê¸°)
                comment_icon = post.select_one(".icon-commenting-o")
                if comment_icon:
                    cmt_text = comment_icon.parent.get_text(strip=True)
                    nums = re.findall(r'\d+', cmt_text)
                    if nums:
                        item["comments"] = int(nums[0])

                # 3. ë‚ ì§œ (ì—¬ê¸°ê°€ í•µì‹¬!)
                # "22ë¶„ ì „" ê°™ì€ í…ìŠ¤íŠ¸ ì°¾ê¸°
                date_tag = post.select_one(".created-at")
                raw_date = ""
                if date_tag:
                    raw_date = date_tag.get_text(strip=True)
                else:
                    # created-atì´ ì—†ìœ¼ë©´ ë©”íƒ€ ì •ë³´ì—ì„œ ì°¾ê¸°
                    meta_tag = post.select_one(".deal-price-meta-info")
                    if meta_tag:
                        raw_date = meta_tag.get_text(strip=True)

                # ğŸ”¥ [ë‚ ì§œ ë³€í™˜ ë§ˆë²•]
                # 'ì „'ì´ë‚˜ 'ë°©ê¸ˆ'ì´ ìˆìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ê°•ì œë¡œ ë¶™ì—¬ì¤Œ
                if any(x in raw_date for x in ["ë°©ê¸ˆ", "ë¶„ ì „", "ì‹œê°„ ì „", "ì´ˆ ì „"]):
                    item["date_text"] = f"{today_str} ({raw_date})"
                else:
                    item["date_text"] = raw_date

                all_items.append(item)

            except Exception as e:
                continue

        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(all_items)}ê°œ (ì•Œêµ¬ëª¬)")
        return json.dumps(all_items, ensure_ascii=False)

    except Exception as e:
        return json.dumps({"error": f"ì•Œêµ¬ëª¬ ì ‘ì† ì‹¤íŒ¨: {e}"}, ensure_ascii=False)

@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ëŒ“ê¸€ ì˜ì—­ ê¸ê¸°
        elements = soup.select(".post-content")
        if not elements:
            elements = soup.select(".comment-list")
            
        content = "\n".join([f"- {el.get_text(strip=True)[:200]}" for el in elements])
        if not content: return "ë‚´ìš© ì—†ìŒ"
        return content[:3000]
    except Exception as e:
        return f"ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()
