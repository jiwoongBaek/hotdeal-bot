# íŒŒì¼ê²½ë¡œ: /home/baek828/hotdeal-bot/server.py
from mcp.server.fastmcp import FastMCP
import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
from datetime import datetime

# ì•Œêµ¬ëª¬ ì£¼ì†Œ
ALGUMON_URL = "https://algumon.com"
mcp = FastMCP("OmniAnalyst")

@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ì•Œêµ¬ëª¬ í•«ë”œ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
    print(f"ğŸ” [ì•Œêµ¬ëª¬] ë¦¬ìŠ¤íŠ¸ ìŠ¤ìº” ì¤‘...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        resp = requests.get(ALGUMON_URL, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_items = []
        today_str = datetime.now().strftime("%m/%d")
        
        products = soup.select(".product-body")
        
        for post in products[:25]:
            try:
                item = {
                    "site": "ì•Œêµ¬ëª¬",
                    "title": "",
                    "comments": 0,
                    "link": "",
                    "date_text": "",
                    "content_selector": "AUTO"
                }

                # ì œëª© & ë§í¬
                title_tag = post.select_one(".deal-title .item-name a")
                if title_tag:
                    item["title"] = title_tag.get_text(strip=True)
                    item["link"] = urljoin(ALGUMON_URL, title_tag.get('href'))
                else: continue

                # ëŒ“ê¸€ ìˆ˜
                comment_icon = post.select_one(".icon-commenting-o")
                if comment_icon:
                    cmt_text = comment_icon.parent.get_text(strip=True)
                    nums = re.findall(r'\d+', cmt_text)
                    if nums: item["comments"] = int(nums[0])

                # ë‚ ì§œ ì •ì œ
                raw_text = ""
                date_tag = post.select_one(".created-at")
                if date_tag: raw_text = date_tag.get_text(strip=True)
                else:
                    meta_tag = post.select_one(".deal-price-meta-info")
                    if meta_tag: raw_text = meta_tag.get_text(strip=True)

                clean_date = ""
                time_match = re.search(r'(\d+ë¶„\s*ì „|\d+ì‹œê°„\s*ì „|ë°©ê¸ˆ|\d+ì´ˆ\s*ì „|\d{2}-\d{2}|\d{2}/\d{2})', raw_text)
                if time_match: clean_date = time_match.group(1)
                else:
                    parts = raw_text.split()
                    if parts: clean_date = parts[-1]

                if any(x in clean_date for x in ["ë°©ê¸ˆ", "ë¶„", "ì‹œê°„", "ì´ˆ"]):
                    item["date_text"] = f"{today_str} ({clean_date})"
                else:
                    item["date_text"] = clean_date

                all_items.append(item)
            except: continue

        print(f"âœ… ë¦¬ìŠ¤íŠ¸ í™•ë³´: {len(all_items)}ê°œ")
        return json.dumps(all_items, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"ì•Œêµ¬ëª¬ ì ‘ì† ì‹¤íŒ¨: {e}"}, ensure_ascii=False)


@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    """ì‚¬ì´íŠ¸ë³„ ëŒ“ê¸€/ë³¸ë¬¸ ìˆ˜ì§‘"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://algumon.com/'
        }
        
        resp = requests.get(url, headers=headers, timeout=10)
        
        # ì¸ì½”ë”© ë³´ì •
        if "ppomppu.co.kr" in url:
            resp.encoding = 'cp949'
        else:
            resp.encoding = resp.apparent_encoding 
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        final_url = resp.url
        print(f"   ğŸ‘‰ [ì™¸ë¶€ ì ‘ì†] {final_url[:30]}...")

        comments = []
        
        if "ppomppu.co.kr" in final_url:
            comments = soup.select(".han-comment, .comment_wrapper, #quote, .list_comment")
            if not comments: comments = soup.select(".board-contents")
        elif "quasarzone.com" in final_url:
            comments = soup.select(".comment-content")
        elif "ruliweb.com" in final_url:
            comments = soup.select(".comment_view, .board_main_view")
        elif "fmkorea.com" in final_url:
            comments = soup.select(".comment-content, .xe_content")
        elif "arca.live" in final_url:
            comments = soup.select(".comment-content, .article-content")
        else:
            comments = soup.select(".comment, .review, .reply, .list-group-item")

        extracted_text = []
        for el in comments:
            text = el.get_text(strip=True)
            if text: extracted_text.append(f"- {text}")
            
        result = "\n".join(extracted_text)
        
        if not result:
            body_text = soup.get_text(strip=True)[:1000]
            return f"[ëŒ“ê¸€ ì°¾ê¸° ì‹¤íŒ¨, ë³¸ë¬¸ ìš”ì•½]\n{body_text}"
            
        return f"[ìˆ˜ì§‘ ì„±ê³µ]\n{result[:3000]}"

    except Exception as e:
        return f"ì ‘ì† ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()
