from mcp.server.fastmcp import FastMCP
import requests
from bs4 import BeautifulSoup
import json
import re
import sys 
from urllib.parse import urljoin
from datetime import datetime

ALGUMON_URL = "https://algumon.com"
mcp = FastMCP("OmniAnalyst")

# ğŸ” ë¡œê·¸ ì „ìš© í•¨ìˆ˜ (í†µì‹  ë°©í•´ ì•ˆ í•¨)
def log(msg):
    sys.stderr.write(f"{msg}\n")
    sys.stderr.flush()

@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ì•Œêµ¬ëª¬ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
    log(f"ğŸ” [ì•Œêµ¬ëª¬] ë¦¬ìŠ¤íŠ¸ ìŠ¤ìº” ì‹œì‘...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        resp = requests.get(ALGUMON_URL, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_items = []
        today_str = datetime.now().strftime("%m/%d")
        
        products = soup.select(".product-body")
        
        for post in products[:30]: 
            try:
                item = {
                    "site": "ì•Œêµ¬ëª¬",
                    "title": "",
                    "comments": 0,
                    "link": "",
                    "date_text": "",
                    "content_selector": "AUTO"
                }

                title_tag = post.select_one(".deal-title .item-name a")
                if title_tag:
                    item["title"] = title_tag.get_text(strip=True)
                    item["link"] = urljoin(ALGUMON_URL, title_tag.get('href'))
                else: continue

                comment_icon = post.select_one(".icon-commenting-o")
                if comment_icon:
                    cmt_text = comment_icon.parent.get_text(strip=True)
                    nums = re.findall(r'\d+', cmt_text)
                    if nums: item["comments"] = int(nums[0])

                raw_text = ""
                date_tag = post.select_one(".created-at")
                if date_tag: raw_text = date_tag.get_text(strip=True)
                else:
                    meta_tag = post.select_one(".deal-price-meta-info")
                    if meta_tag: raw_text = meta_tag.get_text(strip=True)

                clean_date = ""
                time_match = re.search(r'(\d+ë¶„\s*ì „|\d+ì‹œê°„\s*ì „|ë°©ê¸ˆ|\d+ì´ˆ\s*ì „|\d{2}-\d{2}|\d{2}/\d{2})', raw_text)
                if time_match: clean_date = time_match.group(1)
                else:
                    parts = raw_text.split()
                    if parts: clean_date = parts[-1]

                if any(x in clean_date for x in ["ë°©ê¸ˆ", "ë¶„", "ì‹œê°„", "ì´ˆ"]):
                    item["date_text"] = f"{today_str} ({clean_date})"
                else:
                    item["date_text"] = clean_date

                all_items.append(item)
            except: continue

        log(f"âœ… ë¦¬ìŠ¤íŠ¸ í™•ë³´: {len(all_items)}ê°œ")
        return json.dumps(all_items, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"ì•Œêµ¬ëª¬ ì ‘ì† ì‹¤íŒ¨: {e}"}, ensure_ascii=False)


@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    """ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì  ë° ë³¸ë¬¸ ìˆ˜ì§‘"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://algumon.com/'
        }
        
        session = requests.Session()
        resp = session.get(url, headers=headers, timeout=10)
        
        # ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€
        if "ì´ë™ì¤‘" in resp.text or "redirect" in resp.url or "refresh" in resp.text.lower():
            log("   â†ªï¸ ëŒ€ê¸° í˜ì´ì§€ ê°ì§€! ì§„ì§œ ì£¼ì†Œ ì¶”ì  ì¤‘...")
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            meta = soup.find("meta", attrs={"http-equiv": "refresh"})
            new_url = None
            
            if meta:
                content = meta.get("content", "")
                match = re.search(r"url=([^;'\"]+)", content, re.IGNORECASE)
                if match: new_url = match.group(1)
            
            if not new_url:
                match = re.search(r"location\.href\s*=\s*['\"]([^'\"]+)['\"]", resp.text)
                if match: new_url = match.group(1)
                
            if new_url:
                log(f"   ğŸ‘‰ ì§„ì§œ ëª©ì ì§€ ë°œê²¬: {new_url[:40]}...")
                resp = session.get(new_url, headers=headers, timeout=10)

        final_url = resp.url
        log(f"   âœ… ìµœì¢… ì ‘ì†: {final_url[:30]}...")
        
        if "ppomppu.co.kr" in final_url:
            resp.encoding = 'cp949'
        else:
            resp.encoding = resp.apparent_encoding 
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ëŒ“ê¸€ ì°¾ê¸°
        extracted_text = []
        selectors = [
            ".han-comment", ".comment_wrapper", "#quote", ".list_comment", 
            ".comment-content", ".comment_view", ".xe_content", 
            ".reply", ".review", ".comment", ".list-group-item"
        ]
        
        for sel in selectors:
            found = soup.select(sel)
            if found:
                for el in found:
                    t = el.get_text(strip=True)
                    if t: extracted_text.append(f"- {t}")
        
        # ëŒ“ê¸€ ì—†ìœ¼ë©´ ë³¸ë¬¸
        if not extracted_text:
            log("   âš ï¸ ëŒ“ê¸€ ì˜ì—­ ì—†ìŒ -> ë³¸ë¬¸ ì „ì²´ ìˆ˜ì§‘")
            for s in soup(["script", "style", "iframe", "header", "footer", "nav"]):
                s.extract()
            full_text = soup.get_text(separator="\n", strip=True)
            full_text = re.sub(r'\n+', '\n', full_text)
            return f"[ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„]\n{full_text[:3500]}"

        return f"[ëŒ“ê¸€ ìˆ˜ì§‘ ì„±ê³µ]\n" + "\n".join(extracted_text[:50])

    except Exception as e:
        return f"ì ‘ì† ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()
