from mcp.server.fastmcp import FastMCP
import sqlite3
import requests
from bs4 import BeautifulSoup
import json
import os
import re
from urllib.parse import urljoin
from datetime import datetime

ALGUMON_URL = "https://algumon.com"
DB_PATH = "/data/config.db"
mcp = FastMCP("OmniAnalyst")

def init_db():
    os.makedirs("/data", exist_ok=True)ã…from mcp.server.fastmcp import FastMCP
import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
from datetime import datetime

# ì•Œêµ¬ëª¬ ì£¼ì†Œ
ALGUMON_URL = "https://algumon.com"
mcp = FastMCP("OmniAnalyst")

# --- ë„êµ¬ë“¤ ---
@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    print(f"ğŸ” [ì•Œêµ¬ëª¬] ë¦¬ìŠ¤íŠ¸ ìŠ¤ìº” ì¤‘...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        resp = requests.get(ALGUMON_URL, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_items = []
        today_str = datetime.now().strftime("%m/%d")
        
        products = soup.select(".product-body")
        
        for post in products[:25]:
            try:
                item = {
                    "site": "ì•Œêµ¬ëª¬",
                    "title": "",
                    "comments": 0,
                    "link": "",
                    "date_text": "",
                    "content_selector": "AUTO"
                }

                # ì œëª© & ë§í¬
                title_tag = post.select_one(".deal-title .item-name a")
                if title_tag:
                    item["title"] = title_tag.get_text(strip=True)
                    item["link"] = urljoin(ALGUMON_URL, title_tag.get('href'))
                else: continue

                # ëŒ“ê¸€ ìˆ˜
                comment_icon = post.select_one(".icon-commenting-o")
                if comment_icon:
                    cmt_text = comment_icon.parent.get_text(strip=True)
                    nums = re.findall(r'\d+', cmt_text)
                    if nums: item["comments"] = int(nums[0])

                # ë‚ ì§œ ì •ì œ
                raw_text = ""
                date_tag = post.select_one(".created-at")
                if date_tag: raw_text = date_tag.get_text(strip=True)
                else:
                    meta_tag = post.select_one(".deal-price-meta-info")
                    if meta_tag: raw_text = meta_tag.get_text(strip=True)

                clean_date = ""
                time_match = re.search(r'(\d+ë¶„\s*ì „|\d+ì‹œê°„\s*ì „|ë°©ê¸ˆ|\d+ì´ˆ\s*ì „|\d{2}-\d{2}|\d{2}/\d{2})', raw_text)
                if time_match: clean_date = time_match.group(1)
                else:
                    parts = raw_text.split()
                    if parts: clean_date = parts[-1]

                if any(x in clean_date for x in ["ë°©ê¸ˆ", "ë¶„", "ì‹œê°„", "ì´ˆ"]):
                    item["date_text"] = f"{today_str} ({clean_date})"
                else:
                    item["date_text"] = clean_date

                all_items.append(item)
            except: continue

        print(f"âœ… ë¦¬ìŠ¤íŠ¸ í™•ë³´: {len(all_items)}ê°œ")
        return json.dumps(all_items, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"ì•Œêµ¬ëª¬ ì ‘ì† ì‹¤íŒ¨: {e}"}, ensure_ascii=False)


@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    """ì‚¬ì´íŠ¸ë³„ ëŒ“ê¸€/ë³¸ë¬¸ ìˆ˜ì§‘ (ê°•í™”ëœ ë²„ì „)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://algumon.com/'
        }
        
        resp = requests.get(url, headers=headers, timeout=10)
        
        # ë½ë¿Œìš© ì¸ì½”ë”© ê°•ì œ ë³´ì • (EUC-KR ì´ìŠˆ í•´ê²°)
        if "ppomppu.co.kr" in url:
            resp.encoding = 'cp949' # euc-krì˜ í™•ì¥
        else:
            resp.encoding = resp.apparent_encoding 
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        final_url = resp.url
        print(f"   ğŸ‘‰ [ì™¸ë¶€ ì ‘ì†] {final_url[:30]}...")

        comments = []
        
        # --- ì‚¬ì´íŠ¸ë³„ ë§ì¶¤ ì„ íƒì ---
        if "ppomppu.co.kr" in final_url:
            # ë½ë¿Œ: ëŒ“ê¸€ + ë³¸ë¬¸ ì½”ë©˜íŠ¸
            comments = soup.select(".han-comment, .comment_wrapper, #quote, .list_comment")
            if not comments: # ëŒ“ê¸€ ì—†ìœ¼ë©´ ë³¸ë¬¸ ë‚´ìš©ì´ë¼ë„ ê¸ìŒ
                comments = soup.select(".board-contents")
                
        elif "quasarzone.com" in final_url:
            comments = soup.select(".comment-content")
            
        elif "ruliweb.com" in final_url:
            comments = soup.select(".comment_view, .board_main_view")
            
        elif "fmkorea.com" in final_url:
            comments = soup.select(".comment-content, .xe_content")
            
        elif "arca.live" in final_url:
            comments = soup.select(".comment-content, .article-content")
            
        else:
            # ê·¸ ì™¸ ì‚¬ì´íŠ¸ (ì‡¼í•‘ëª° ë“±): ì¼ë°˜ì ì¸ ëŒ“ê¸€ í´ë˜ìŠ¤ ì‹œë„
            comments = soup.select(".comment, .review, .reply, .list-group-item")

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = []
        for el in comments:
            text = el.get_text(strip=True)
            if text: extracted_text.append(f"- {text}")
            
        result = "\n".join(extracted_text)
        
        # ì • ëª» ì°¾ì•˜ìœ¼ë©´ í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ ì¼ë¶€ë¼ë„ ë°˜í™˜ (AI íŒë‹¨ìš©)
        if not result:
            body_text = soup.get_text(strip=True)[:1000]
            return f"[ëŒ“ê¸€ ì°¾ê¸° ì‹¤íŒ¨, ë³¸ë¬¸ ìš”ì•½]\n{body_text}"
            
        return f"[ìˆ˜ì§‘ ì„±ê³µ]\n{result[:3000]}"

    except Exception as e:
        return f"ì ‘ì† ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()
    conn = sqlite3.connect(DB_PATH)
    conn.execute('CREATE TABLE IF NOT EXISTS environments (name TEXT PRIMARY KEY, description TEXT)')
    conn.execute('CREATE TABLE IF NOT EXISTS sites (id INTEGER PRIMARY KEY, env_name TEXT, site_name TEXT)')
    conn.close()

init_db()

@mcp.tool()
def create_environment(name: str, description: str = "") -> str:
    return "âœ… ì•Œêµ¬ëª¬ ì „ìš© ëª¨ë“œ"

@mcp.tool()
def add_board_to_env(env_name: str, site_name: str, board_url: str, title_selector: str, comment_selector: str, content_selector: str, date_selector: str, link_selector: str = "") -> str:
    return "âœ… ì„¤ì • ë¶ˆí•„ìš” (ìë™ ê°ì§€ ëª¨ë“œ)"

@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ì•Œêµ¬ëª¬ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (ë‚ ì§œ ì •ì œ í¬í•¨)"""
    print(f"ğŸ” [ì•Œêµ¬ëª¬] ë¦¬ìŠ¤íŠ¸ ìŠ¤ìº” ì¤‘...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        resp = requests.get(ALGUMON_URL, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_items = []
        today_str = datetime.now().strftime("%m/%d")
        
        products = soup.select(".product-body")
        
        for post in products[:25]:
            try:
                item = {
                    "site": "ì•Œêµ¬ëª¬",
                    "title": "",
                    "comments": 0,
                    "link": "",
                    "date_text": "",
                    "content_selector": "AUTO" # ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ í‘œì‹œ
                }

                # ì œëª© & ë§í¬
                title_tag = post.select_one(".deal-title .item-name a")
                if title_tag:
                    item["title"] = title_tag.get_text(strip=True)
                    item["link"] = urljoin(ALGUMON_URL, title_tag.get('href'))
                else: continue

                # ëŒ“ê¸€ ìˆ˜
                comment_icon = post.select_one(".icon-commenting-o")
                if comment_icon:
                    cmt_text = comment_icon.parent.get_text(strip=True)
                    nums = re.findall(r'\d+', cmt_text)
                    if nums: item["comments"] = int(nums[0])

                # ë‚ ì§œ ì •ì œ
                raw_text = ""
                date_tag = post.select_one(".created-at")
                if date_tag: raw_text = date_tag.get_text(strip=True)
                else:
                    meta_tag = post.select_one(".deal-price-meta-info")
                    if meta_tag: raw_text = meta_tag.get_text(strip=True)

                clean_date = ""
                time_match = re.search(r'(\d+ë¶„\s*ì „|\d+ì‹œê°„\s*ì „|ë°©ê¸ˆ|\d+ì´ˆ\s*ì „|\d{2}-\d{2}|\d{2}/\d{2})', raw_text)
                if time_match: clean_date = time_match.group(1)
                else:
                    parts = raw_text.split()
                    if parts: clean_date = parts[-1]

                if any(x in clean_date for x in ["ë°©ê¸ˆ", "ë¶„", "ì‹œê°„", "ì´ˆ"]):
                    item["date_text"] = f"{today_str} ({clean_date})"
                else:
                    item["date_text"] = clean_date

                all_items.append(item)
            except: continue

        print(f"âœ… ë¦¬ìŠ¤íŠ¸ í™•ë³´: {len(all_items)}ê°œ")
        return json.dumps(all_items, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"ì•Œêµ¬ëª¬ ì ‘ì† ì‹¤íŒ¨: {e}"}, ensure_ascii=False)


# ğŸ”¥ [í•µì‹¬ ê¸°ëŠ¥] ì‚¬ì´íŠ¸ë³„ ëŒ“ê¸€ ìˆ˜ì§‘ê¸°
@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    """ë§í¬ë¥¼ íƒ€ê³  ë“¤ì–´ê°€ì„œ ì‚¬ì´íŠ¸ë³„ë¡œ ëŒ“ê¸€ì„ ê¸ì–´ì˜µë‹ˆë‹¤."""
    try:
        # 1. í—¤ë” ì„¤ì • (ì°¨ë‹¨ ë°©ì§€ìš©)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://algumon.com/'
        }
        
        # 2. ì ‘ì† (ë¦¬ë‹¤ì´ë ‰íŠ¸ ìë™ ì¶”ì )
        # ì•Œêµ¬ëª¬ ë§í¬ -> ì‹¤ì œ ì‚¬ì´íŠ¸(ë½ë¿Œ ë“±)ë¡œ ì´ë™ë¨
        resp = requests.get(url, headers=headers, timeout=10)
        
        # ì¸ì½”ë”© ìë™ ë³´ì • (ë½ë¿Œ ë“±ì—ì„œ í•œê¸€ ê¹¨ì§ ë°©ì§€)
        resp.encoding = resp.apparent_encoding 
        
        final_url = resp.url
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        print(f"   ğŸ‘‰ [ì™¸ë¶€ ì ‘ì†] {final_url[:40]}... (ì‚¬ì´íŠ¸ íŒë… ì¤‘)")

        # 3. ì‚¬ì´íŠ¸ë³„ ëŒ“ê¸€ ì„ íƒì ë§¤í•‘
        comments = []
        
        if "ppomppu.co.kr" in final_url: # ë½ë¿Œ
            # ë½ë¿ŒëŠ” ëŒ“ê¸€ êµ¬ì¡°ê°€ ë‹¤ì–‘í•¨ (ì¼ë°˜/ëª¨ë°”ì¼/ì•±)
            # ì£¼ìš” ëŒ“ê¸€ ì˜ì—­ë“¤ ì‹œë„
            selectors = [".han-comment", ".comment_wrapper", "#quote", ".comment-content"]
            for sel in selectors:
                found = soup.select(sel)
                if found:
                    comments = found; break
                    
        elif "quasarzone.com" in final_url: # í€˜ì´ì‚¬ì¡´
            comments = soup.select(".comment-content")
            
        elif "ruliweb.com" in final_url: # ë£¨ë¦¬ì›¹
            comments = soup.select(".comment_view")
            
        elif "fmkorea.com" in final_url: # í¨ì½”
            comments = soup.select(".comment-content")
            
        elif "arca.live" in final_url: # ì•„ì¹´ë¼ì´ë¸Œ
            comments = soup.select(".comment-content")
            
        else: # ê·¸ ì™¸ ì‚¬ì´íŠ¸ (ë„¤ì´ë²„ëª°, Gë§ˆì¼“ ë“± ì‡¼í•‘ëª° ìì²´ì¼ ê²½ìš°)
            # ì¼ë°˜ì ì¸ ëŒ“ê¸€ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ì°ì–´ë³´ê¸°
            comments = soup.select(".comment, .review, .reply, .list-group-item")

        # 4. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
        extracted_text = []
        for el in comments:
            text = el.get_text(strip=True)
            if text: extracted_text.append(f"- {text}")
            
        result = "\n".join(extracted_text)
        
        if not result:
            return "âš ï¸ ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë‹¤ë¥´ê±°ë‚˜ ëŒ“ê¸€ì´ ì—†ìŒ)"
            
        return f"[ëŒ“ê¸€ ìˆ˜ì§‘ ì„±ê³µ]\n{result[:3000]}" # ë„ˆë¬´ ê¸¸ë©´ ìë¦„

    except Exception as e:
        return f"ìƒì„¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()
