from mcp.server.fastmcp import FastMCP
import sqlite3
import requests
from bs4 import BeautifulSoup
import json
import os
import re
from urllib.parse import urljoin
from datetime import datetime

# ì•Œêµ¬ëª¬ ì£¼ì†Œ
ALGUMON_URL = "https://algumon.com"
DB_PATH = "/data/config.db"
mcp = FastMCP("OmniAnalyst")

def init_db():
    os.makedirs("/data", exist_ok=True)
    # DB ê´€ë ¨ ì½”ë“œëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
    conn = sqlite3.connect(DB_PATH)
    conn.execute('CREATE TABLE IF NOT EXISTS environments (name TEXT PRIMARY KEY, description TEXT)')
    conn.execute('CREATE TABLE IF NOT EXISTS sites (id INTEGER PRIMARY KEY, env_name TEXT, site_name TEXT)')
    conn.close()

init_db()

@mcp.tool()
def create_environment(name: str, description: str = "") -> str:
    return "âœ… ì•Œêµ¬ëª¬ ì „ìš© ëª¨ë“œì…ë‹ˆë‹¤."

@mcp.tool()
def add_board_to_env(env_name: str, site_name: str, board_url: str, title_selector: str, comment_selector: str, content_selector: str, date_selector: str, link_selector: str = "") -> str:
    return "âœ… ì•Œêµ¬ëª¬ ì „ìš© ëª¨ë“œë¼ ì„¤ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤."

@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ì•Œêµ¬ëª¬ ì „ìš© íŒŒì„œ (ë‚ ì§œ ì •ì œ ê¸°ëŠ¥ ê°•í™”)"""
    print(f"ğŸ” [ì•Œêµ¬ëª¬] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        resp = requests.get(ALGUMON_URL, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_items = []
        today_str = datetime.now().strftime("%m/%d") # ì˜ˆ: "12/17"
        
        # .product-body í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  ìš”ì†Œ ìŠ¤ìº”
        products = soup.select(".product-body")
        
        for post in products[:25]: # ìƒìœ„ 25ê°œë§Œ
            try:
                item = {
                    "site": "ì•Œêµ¬ëª¬",
                    "title": "",
                    "comments": 0,
                    "link": "",
                    "date_text": "",
                    "content_selector": ".post-content"
                }

                # 1. ì œëª© & ë§í¬
                title_tag = post.select_one(".deal-title .item-name a")
                if title_tag:
                    item["title"] = title_tag.get_text(strip=True)
                    item["link"] = urljoin(ALGUMON_URL, title_tag.get('href'))
                else:
                    continue

                # 2. ëŒ“ê¸€ ìˆ˜ (ì•„ì´ì½˜ ì˜† ìˆ«ì ì°¾ê¸°)
                comment_icon = post.select_one(".icon-commenting-o")
                if comment_icon:
                    cmt_text = comment_icon.parent.get_text(strip=True)
                    nums = re.findall(r'\d+', cmt_text)
                    if nums:
                        item["comments"] = int(nums[0])

                # 3. ë‚ ì§œ (ì—¬ê¸°ê°€ í•µì‹¬ ê°œì„  í¬ì¸íŠ¸!)
                raw_text = ""
                
                # (A) .created-at íƒœê·¸ê°€ ìˆìœ¼ë©´ ìµœìš°ì„ 
                date_tag = post.select_one(".created-at")
                if date_tag:
                    raw_text = date_tag.get_text(strip=True)
                else:
                    # (B) ì—†ìœ¼ë©´ ë©”íƒ€ ì •ë³´ ì „ì²´ì—ì„œ ì°¾ê¸°
                    meta_tag = post.select_one(".deal-price-meta-info")
                    if meta_tag:
                        raw_text = meta_tag.get_text(strip=True) # ì—¬ê¸°ì— ë°°ì†¡ë¹„ ë“± ì¡ë™ì‚¬ë‹ˆê°€ ì„ì—¬ ìˆìŒ

                # ğŸ”¥ [ë‚ ì§œ ì •ì œ ë§ˆë²•] ì •ê·œì‹ìœ¼ë¡œ ì‹œê°„ íŒ¨í„´ë§Œ ì¶”ì¶œ
                # ì˜ˆ: "35ë¶„ ì „", "1ì‹œê°„ ì „", "ë°©ê¸ˆ", "12-17" ë“±ì„ ì°¾ìŒ
                clean_date = ""
                time_match = re.search(r'(\d+ë¶„\s*ì „|\d+ì‹œê°„\s*ì „|ë°©ê¸ˆ|\d+ì´ˆ\s*ì „|\d{2}-\d{2}|\d{2}/\d{2})', raw_text)
                
                if time_match:
                    clean_date = time_match.group(1)
                else:
                    # ì •ê·œì‹ ì‹¤íŒ¨ ì‹œ, í…ìŠ¤íŠ¸ì˜ ë§¨ ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ê°€ì ¸ì˜´ (ë³´í†µ ë‚ ì§œê°€ ëì— ìˆìŒ)
                    parts = raw_text.split()
                    if parts: clean_date = parts[-1]

                # ìµœì¢… ë‚ ì§œ í¬ë§·íŒ…
                if any(x in clean_date for x in ["ë°©ê¸ˆ", "ë¶„", "ì‹œê°„", "ì´ˆ"]):
                    item["date_text"] = f"{today_str} ({clean_date})"
                else:
                    item["date_text"] = clean_date

                all_items.append(item)

            except Exception as e:
                continue

        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(all_items)}ê°œ (ì•Œêµ¬ëª¬)")
        return json.dumps(all_items, ensure_ascii=False)

    except Exception as e:
        return json.dumps({"error": f"ì•Œêµ¬ëª¬ ì ‘ì† ì‹¤íŒ¨: {e}"}, ensure_ascii=False)

@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        elements = soup.select(".post-content")
        if not elements:
            elements = soup.select(".comment-list")
            
        content = "\n".join([f"- {el.get_text(strip=True)[:200]}" for el in elements])
        if not content: return "ë‚´ìš© ì—†ìŒ"
        return content[:3000]
    except Exception as e:
        return f"ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()
