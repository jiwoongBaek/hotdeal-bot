# server.py (ë‚ ì§œ í•„í„°ë§ + ëŒ“ê¸€ ë¶„ì„ ë²„ì „)
from mcp.server.fastmcp import FastMCP
import sqlite3
import requests
from bs4 import BeautifulSoup
import json
import os
import re
from urllib.parse import urljoin

DB_PATH = "/data/config.db"
mcp = FastMCP("OmniAnalyst")

def init_db():
    os.makedirs("/data", exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    conn.execute('''
        CREATE TABLE IF NOT EXISTS environments (name TEXT PRIMARY KEY, description TEXT)
    ''')
    # ğŸŒŸ date_selector ì»¬ëŸ¼ ì¶”ê°€ (ë‚ ì§œ í•„í„°ë§ìš©)
    conn.execute('''
        CREATE TABLE IF NOT EXISTS sites (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            env_name TEXT,
            site_name TEXT,
            board_url TEXT,
            title_selector TEXT,
            comment_selector TEXT,
            link_selector TEXT,
            content_selector TEXT, -- ì´ì œë¶€í„° ì´ê±´ 'ëŒ“ê¸€ ì˜ì—­'ì„ ê¸ëŠ” ìš©ë„ë¡œ ì”ë‹ˆë‹¤
            date_selector TEXT,    -- [ì‹ ê·œ] ë¦¬ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ/ì‹œê°„ ìœ„ì¹˜
            FOREIGN KEY(env_name) REFERENCES environments(name)
        )
    ''')
    conn.commit()
    conn.close()

init_db()

# --- âš™ï¸ ì„¤ì • ë„êµ¬ ---
@mcp.tool()
def create_environment(name: str, description: str = "") -> str:
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute("INSERT INTO environments VALUES (?, ?)", (name, description))
        conn.commit()
        return f"âœ… í™˜ê²½ ìƒì„±: {name}"
    except:
        return "âš ï¸ ì´ë¯¸ ì¡´ì¬í•¨"
    finally:
        conn.close()

@mcp.tool()
def add_board_to_env(env_name: str, site_name: str, board_url: str, title_selector: str, comment_selector: str, content_selector: str, date_selector: str, link_selector: str = "") -> str:
    """ì‚¬ì´íŠ¸ ì¶”ê°€ (ë‚ ì§œ ì„ íƒì í¬í•¨)"""
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute(
            "INSERT INTO sites (env_name, site_name, board_url, title_selector, comment_selector, link_selector, content_selector, date_selector) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
            (env_name, site_name, board_url, title_selector, comment_selector, link_selector, content_selector, date_selector)
        )
        conn.commit()
        return f"âœ… ì‚¬ì´íŠ¸ ì¶”ê°€ ì™„ë£Œ: {site_name}"
    except Exception as e:
        return f"âŒ ì‹¤íŒ¨: {e}"
    finally:
        conn.close()

# --- ğŸ” ìˆ˜ì§‘ ë„êµ¬ ---
@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (ë‚ ì§œ ì •ë³´ í¬í•¨)"""
    conn = sqlite3.connect(DB_PATH)
    sites = conn.execute("SELECT site_name, board_url, title_selector, comment_selector, link_selector, content_selector, date_selector FROM sites WHERE env_name = ?", (env_name,)).fetchall()
    conn.close()

    if not sites: return json.dumps({"error": "ë“±ë¡ëœ ì‚¬ì´íŠ¸ ì—†ìŒ"})

    all_items = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8'
    }

    for site_name, url, t_sel, c_sel, l_sel, cont_sel, d_sel in sites:
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            resp.encoding = resp.apparent_encoding
            soup = BeautifulSoup(resp.text, 'html.parser')
            titles = soup.select(t_sel)
            
            for t_el in titles[:20]:
                item = {
                    "site": site_name, 
                    "title": t_el.get_text(strip=True), 
                    "comments": 0, 
                    "link": "",
                    "date_text": "", # [ì‹ ê·œ] ë‚ ì§œ í…ìŠ¤íŠ¸
                    "content_selector": cont_sel
                }
                
                # ë§í¬ ì°¾ê¸°
                a_tag = t_el if t_el.name == 'a' else (t_el.select_one(l_sel) if l_sel else t_el.find_parent('a'))
                if a_tag and a_tag.has_attr('href'):
                    item["link"] = urljoin(url, a_tag['href'])

                # ëŒ“ê¸€ ìˆ˜ ì°¾ê¸°
                if c_sel:
                    c_tag = t_el.select_one(c_sel) or (t_el.parent.select_one(c_sel) if t_el.parent else None)
                    if c_tag:
                        nums = re.findall(r'\d+', c_tag.get_text())
                        if nums: item["comments"] = int(nums[0])

                # [ì‹ ê·œ] ë‚ ì§œ ì°¾ê¸°
                if d_sel:
                    d_tag = t_el.select_one(d_sel) or (t_el.parent.select_one(d_sel) if t_el.parent else None)
                    if d_tag:
                        item["date_text"] = d_tag.get_text(strip=True)

                all_items.append(item)
        except Exception as e:
            all_items.append({"error": f"{site_name} ì—ëŸ¬: {e}"})

    return json.dumps(all_items, ensure_ascii=False)

@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    """ê²Œì‹œê¸€ ë§í¬ë¡œ ë“¤ì–´ê°€ì„œ ë‚´ìš©(ì´ì œëŠ” ëŒ“ê¸€ë“¤)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ
        content = ""
        if content_selector:
            # ëŒ“ê¸€ë“¤ì€ ì—¬ëŸ¬ ê°œê°€ ìˆìœ¼ë‹ˆ ëª¨ë‘ ê¸ì–´ì„œ í•©ì¹¨
            elements = soup.select(content_selector)
            content = "\n".join([f"- {el.get_text(strip=True)}" for el in elements])
        
        if not content: return "ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        return content[:3000] # ëŒ“ê¸€ì€ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ 3000ì ì œí•œ
    except Exception as e:
        return f"ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()