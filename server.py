from mcp.server.fastmcp import FastMCP
import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
from datetime import datetime

# ì•Œêµ¬ëª¬ ì£¼ì†Œ
ALGUMON_URL = "https://algumon.com"
mcp = FastMCP("OmniAnalyst")

@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    print(f"ğŸ” [ì•Œêµ¬ëª¬] 1í˜ì´ì§€ ìŠ¤ìº” ì‹œì‘...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        resp = requests.get(ALGUMON_URL, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_items = []
        today_str = datetime.now().strftime("%m/%d")
        
        products = soup.select(".product-body")
        
        for post in products[:30]: 
            try:
                item = {
                    "site": "ì•Œêµ¬ëª¬",
                    "title": "",
                    "comments": 0,
                    "link": "",
                    "date_text": "",
                    "content_selector": "AUTO"
                }

                title_tag = post.select_one(".deal-title .item-name a")
                if title_tag:
                    item["title"] = title_tag.get_text(strip=True)
                    item["link"] = urljoin(ALGUMON_URL, title_tag.get('href'))
                else: continue

                comment_icon = post.select_one(".icon-commenting-o")
                if comment_icon:
                    cmt_text = comment_icon.parent.get_text(strip=True)
                    nums = re.findall(r'\d+', cmt_text)
                    if nums: item["comments"] = int(nums[0])

                raw_text = ""
                date_tag = post.select_one(".created-at")
                if date_tag: raw_text = date_tag.get_text(strip=True)
                else:
                    meta_tag = post.select_one(".deal-price-meta-info")
                    if meta_tag: raw_text = meta_tag.get_text(strip=True)

                clean_date = ""
                time_match = re.search(r'(\d+ë¶„\s*ì „|\d+ì‹œê°„\s*ì „|ë°©ê¸ˆ|\d+ì´ˆ\s*ì „|\d{2}-\d{2}|\d{2}/\d{2})', raw_text)
                if time_match: clean_date = time_match.group(1)
                else:
                    parts = raw_text.split()
                    if parts: clean_date = parts[-1]

                if any(x in clean_date for x in ["ë°©ê¸ˆ", "ë¶„", "ì‹œê°„", "ì´ˆ"]):
                    item["date_text"] = f"{today_str} ({clean_date})"
                else:
                    item["date_text"] = clean_date

                all_items.append(item)
            except: continue

        print(f"âœ… ë¦¬ìŠ¤íŠ¸ í™•ë³´: {len(all_items)}ê°œ")
        return json.dumps(all_items, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"ì•Œêµ¬ëª¬ ì ‘ì† ì‹¤íŒ¨: {e}"}, ensure_ascii=False)


@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    """ì‚¬ì´íŠ¸ ë‚´ìš© ìˆ˜ì§‘ (ê°•ë ¥í•œ í´ë°± ì ìš©)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://algumon.com/'
        }
        
        resp = requests.get(url, headers=headers, timeout=10)
        final_url = resp.url
        print(f"   ğŸ‘‰ ì ‘ì†: {final_url[:40]}...")

        # ì¸ì½”ë”© ë³´ì •
        if "ppomppu.co.kr" in final_url:
            resp.encoding = 'cp949'
        else:
            resp.encoding = resp.apparent_encoding 
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # 1. ëŒ“ê¸€ ì „ìš© êµ¬ì—­ ì‹œë„
        selectors = [
            ".han-comment", ".comment_wrapper", "#quote", ".list_comment", # ë½ë¿Œ
            ".comment-content", ".comment_view", ".xe_content", # í€˜ì´ì‚¬/ë£¨ë¦¬ì›¹
            ".reply", ".review", ".comment", ".list-group-item" 
        ]
        
        extracted_text = []
        for sel in selectors:
            found = soup.select(sel)
            if found:
                for el in found:
                    t = el.get_text(strip=True)
                    extracted_text.append(f"- {t}")
        
        # 2. ëŒ“ê¸€ì´ ì—†ìœ¼ë©´? -> í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ ê¸ì–´ì„œ ë°˜í™˜ (ì ˆëŒ€ ì‹¤íŒ¨ ì—†ìŒ)
        if not extracted_text:
            print("   âš ï¸ ëŒ“ê¸€ ì„ íƒ ì‹¤íŒ¨ -> í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘")
            
            # ìŠ¤í¬ë¦½íŠ¸ ì œê±°
            for s in soup(["script", "style", "iframe", "header", "footer", "nav"]):
                s.extract()
                
            full_text = soup.get_text(separator="\n", strip=True)
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            full_text = re.sub(r'\n+', '\n', full_text)
            
            return f"[ì „ì²´ í˜ì´ì§€ ë‚´ìš©]\n{full_text[:4000]}" # 4000ì ì œí•œ

        return f"[ëŒ“ê¸€ ìˆ˜ì§‘ ì„±ê³µ]\n" + "\n".join(extracted_text[:50])

    except Exception as e:
        return f"ì ‘ì† ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()
