# server.py (ìµœì¢…_ì§„ì§œ_ìµœì¢…_v2.py)
from mcp.server.fastmcp import FastMCP
import sqlite3
import requests
from bs4 import BeautifulSoup
import json
import os
import re
from urllib.parse import urljoin

DB_PATH = "/data/config.db"
mcp = FastMCP("OmniAnalyst")

def init_db():
    os.makedirs("/data", exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    conn.execute('''
        CREATE TABLE IF NOT EXISTS environments (name TEXT PRIMARY KEY, description TEXT)
    ''')
    # ğŸŒŸ content_selector ì»¬ëŸ¼ ì¶”ê°€ë¨ (ë³¸ë¬¸ ê¸ì–´ì˜¤ê¸°ìš©)
    conn.execute('''
        CREATE TABLE IF NOT EXISTS sites (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            env_name TEXT,
            site_name TEXT,
            board_url TEXT,
            title_selector TEXT,
            comment_selector TEXT,
            link_selector TEXT,
            content_selector TEXT, 
            FOREIGN KEY(env_name) REFERENCES environments(name)
        )
    ''')
    conn.commit()
    conn.close()

init_db()

# --- âš™ï¸ ì„¤ì • ë„êµ¬ ---

@mcp.tool()
def create_environment(name: str, description: str = "") -> str:
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute("INSERT INTO environments VALUES (?, ?)", (name, description))
        conn.commit()
        return f"âœ… í™˜ê²½ ìƒì„±: {name}"
    except:
        return "âš ï¸ ì´ë¯¸ ì¡´ì¬í•¨"
    finally:
        conn.close()

# ğŸŒŸ content_selector ì¸ì ì¶”ê°€
@mcp.tool()
def add_board_to_env(env_name: str, site_name: str, board_url: str, title_selector: str, comment_selector: str, content_selector: str, link_selector: str = "") -> str:
    """ì‚¬ì´íŠ¸ ì¶”ê°€ (ë³¸ë¬¸ ì„ íƒì í¬í•¨)"""
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute(
            "INSERT INTO sites (env_name, site_name, board_url, title_selector, comment_selector, link_selector, content_selector) VALUES (?, ?, ?, ?, ?, ?, ?)",
            (env_name, site_name, board_url, title_selector, comment_selector, link_selector, content_selector)
        )
        conn.commit()
        return f"âœ… ì‚¬ì´íŠ¸ ì¶”ê°€ ì™„ë£Œ: {site_name}"
    except Exception as e:
        return f"âŒ ì‹¤íŒ¨: {e}"
    finally:
        conn.close()

# --- ğŸ” ìˆ˜ì§‘ ë„êµ¬ ---

@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ê²Œì‹œíŒ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë³¸ë¬¸ ì„ íƒì ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜)"""
    conn = sqlite3.connect(DB_PATH)
    sites = conn.execute("SELECT site_name, board_url, title_selector, comment_selector, link_selector, content_selector FROM sites WHERE env_name = ?", (env_name,)).fetchall()
    conn.close()

    if not sites: return json.dumps({"error": "ë“±ë¡ëœ ì‚¬ì´íŠ¸ ì—†ìŒ"})

    all_items = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://www.google.com/'
    }

    for site_name, url, t_sel, c_sel, l_sel, cont_sel in sites:
        try:
            resp = requests.get(url, headers=headers, timeout=5)
            soup = BeautifulSoup(resp.text, 'html.parser')
            titles = soup.select(t_sel)
            
            for t_el in titles[:15]:
                item = {
                    "site": site_name, "title": t_el.get_text(strip=True), 
                    "comments": 0, "link": "", 
                    "content_selector": cont_sel # ğŸŒŸ ì¤‘ìš”: ìƒì„¸ í˜ì´ì§€ ê¸ì„ ë•Œ ì“¸ ì„ íƒìë¥¼ ê°™ì´ ì „ë‹¬
                }
                
                # ë§í¬ ì°¾ê¸°
                a_tag = t_el if t_el.name == 'a' else (t_el.select_one(l_sel) if l_sel else t_el.find_parent('a'))
                if a_tag and a_tag.has_attr('href'):
                    item["link"] = urljoin(url, a_tag['href'])

                # ëŒ“ê¸€ ìˆ˜ ì°¾ê¸°
                if c_sel:
                    c_tag = t_el.select_one(c_sel) or (t_el.parent.select_one(c_sel) if t_el.parent else None)
                    if c_tag:
                        nums = re.findall(r'\d+', c_tag.get_text())
                        if nums: item["comments"] = int(nums[0])

                all_items.append(item)
        except Exception as e:
            print(f"List Error {site_name}: {e}")

    return json.dumps(all_items, ensure_ascii=False)

@mcp.tool()
def debug_site(url: str) -> str:
    """í•´ë‹¹ URLì— ì ‘ì†í•´ì„œ ìƒíƒœ ì½”ë“œì™€ HTML ì•ë¶€ë¶„ 500ìë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤."""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        resp = requests.get(url, headers=headers, timeout=10)
        return f"ìƒíƒœì½”ë“œ: {resp.status_code}\në‚´ìš©ì¼ë¶€:\n{resp.text[:500]}"
    except Exception as e:
        return f"ì ‘ì† ì—ëŸ¬: {e}"

# ğŸŒŸ [ì‹ ê·œ] ìƒì„¸ í˜ì´ì§€ ë‚´ìš© ê¸ì–´ì˜¤ê¸° ë„êµ¬
@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ë””ë²„ê¹… ë¡œê·¸ê°€ ì¶”ê°€ëœ ë²„ì „ì…ë‹ˆë‹¤."""
    conn = sqlite3.connect(DB_PATH)
    sites = conn.execute("SELECT site_name, board_url, title_selector, comment_selector, link_selector, content_selector FROM sites WHERE env_name = ?", (env_name,)).fetchall()
    conn.close()

    if not sites: return json.dumps({"error": "ë“±ë¡ëœ ì‚¬ì´íŠ¸ ì—†ìŒ"})

    all_items = []
    # ë½ë¿Œìš© ê°•ë ¥í•œ í—¤ë”
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Referer': 'https://www.google.com/'
    }

    for site_name, url, t_sel, c_sel, l_sel, cont_sel in sites:
        print(f"\n--- [DEBUG] {site_name} ìˆ˜ì§‘ ì‹œì‘ ---")
        print(f"URL: {url}")
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            resp.encoding = resp.apparent_encoding # í•œê¸€ ê¹¨ì§ ë°©ì§€
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ì œëª© ìš”ì†Œë“¤ ì°¾ê¸°
            titles = soup.select(t_sel)
            print(f"ì°¾ì€ ì œëª© ê°œìˆ˜: {len(titles)}")
            
            if len(titles) == 0:
                print(f"âš ï¸ ê²½ê³ : ì œëª© ì„ íƒì({t_sel})ë¡œ ì•„ë¬´ê²ƒë„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                # HTML êµ¬ì¡°ê°€ ê¶ê¸ˆí•˜ë©´ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
                print(f"HTML ì•ë¶€ë¶„: {soup.prettify()[:500]}")
            
            for i, t_el in enumerate(titles[:5]): # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸ ì¶œë ¥
                title_text = t_el.get_text(strip=True)
                print(f"\n[{i+1}] ì œëª©: {title_text}")
                
                # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ ì‹œë„
                comment_count = 0
                if c_sel:
                    c_tag = t_el.select_one(c_sel)
                    # ì œëª© ì•ˆì— ì—†ìœ¼ë©´ ë¶€ëª¨ì˜ í˜•ì œ/ìì‹ì—ì„œ ì°¾ê¸° (ë½ë¿Œ PCë²„ì „ êµ¬ì¡° ëŒ€ì‘)
                    if not c_tag and t_el.parent:
                         c_tag = t_el.parent.select_one(c_sel)
                    
                    if c_tag:
                        c_text = c_tag.get_text(strip=True)
                        print(f"    - ëŒ“ê¸€ ìš”ì†Œ ì°¾ìŒ: '{c_text}'")
                        nums = re.findall(r'\d+', c_text)
                        if nums: 
                            comment_count = int(nums[0])
                    else:
                        print(f"    - ëŒ“ê¸€ ìš”ì†Œ ëª» ì°¾ìŒ (ì„ íƒì: {c_sel})")

                print(f"    -> ìµœì¢… ì¶”ì¶œ ëŒ“ê¸€ ìˆ˜: {comment_count}")

                # ì•„ì´í…œ ì¶”ê°€ (ê¸°ì¡´ ë¡œì§)
                item = {"site": site_name, "title": title_text, "comments": comment_count, "link": "", "content_selector": cont_sel}
                # ë§í¬ ì°¾ê¸° ë¡œì§... (ìƒëµ ì—†ì´ ê¸°ì¡´ëŒ€ë¡œ ë™ì‘)
                a_tag = t_el if t_el.name == 'a' else (t_el.select_one(l_sel) if l_sel else t_el.find_parent('a'))
                if a_tag and a_tag.has_attr('href'):
                    item["link"] = urljoin(url, a_tag['href'])
                all_items.append(item)

        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ {site_name}: {e}")

    return json.dumps(all_items, ensure_ascii=False)