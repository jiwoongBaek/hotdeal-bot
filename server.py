from mcp.server.fastmcp import FastMCP
import sqlite3
import requests
from bs4 import BeautifulSoup
import json
import os
import re
from urllib.parse import urljoin

# ì•Œêµ¬ëª¬ ì£¼ì†Œ (ê³ ì •)
ALGUMON_URL = "https://algumon.com"

DB_PATH = "/data/config.db"
mcp = FastMCP("OmniAnalyst")

# --- ğŸ› ï¸ ì´ˆê¸°í™” (DBëŠ” ì—ëŸ¬ ë°©ì§€ìš©ìœ¼ë¡œ ì‚´ë ¤ë‘ ) ---
def init_db():
    os.makedirs("/data", exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    conn.execute('CREATE TABLE IF NOT EXISTS environments (name TEXT PRIMARY KEY, description TEXT)')
    conn.execute('CREATE TABLE IF NOT EXISTS sites (id INTEGER PRIMARY KEY, env_name TEXT, site_name TEXT)')
    conn.commit()
    conn.close()

init_db()

@mcp.tool()
def create_environment(name: str, description: str = "") -> str:
    return f"âœ… í™˜ê²½ '{name}' ì„¤ì •ë¨ (ì•Œêµ¬ëª¬ ì „ìš© ëª¨ë“œ)"

@mcp.tool()
def add_board_to_env(env_name: str, site_name: str, board_url: str, title_selector: str, comment_selector: str, content_selector: str, date_selector: str, link_selector: str = "") -> str:
    return "âœ… (ì•Œêµ¬ëª¬ ì „ìš© ëª¨ë“œë¼ ì„¤ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ë°”ë¡œ monitor ëª…ë ¹ì–´ë¥¼ ì“°ì„¸ìš”!)"

# --- ğŸ” ì•Œêµ¬ëª¬ ì „ìš© ìˆ˜ì§‘ê¸° (í•µì‹¬) ---
@mcp.tool()
def fetch_board_items(env_name: str) -> str:
    """ì•Œêµ¬ëª¬ í•«ë”œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ìš© íŒŒì„œë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    print(f"ğŸ” [ì•Œêµ¬ëª¬] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        resp = requests.get(ALGUMON_URL, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_items = []
        
        # 1. ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ì „ì²´ ê°€ì ¸ì˜¤ê¸° (li.post-item)
        post_items = soup.select("li.post-item")
        
        for post in post_items[:25]: # ìƒìœ„ 25ê°œë§Œ
            try:
                item = {
                    "site": "ì•Œêµ¬ëª¬",
                    "title": "",
                    "comments": 0,
                    "link": "",
                    "date_text": "",
                    "content_selector": ".post-content" # ë³¸ë¬¸(ëŒ“ê¸€) ê¸ì–´ì˜¬ ë•Œ ì“¸ ì˜ì—­
                }

                # (1) ì œëª© & ë§í¬ ì¶”ì¶œ
                # .deal-title ì•ˆì— ìˆëŠ” ë§í¬(a)ê°€ ì§„ì§œ ì œëª©ì„
                title_tag = post.select_one(".deal-title .item-name a")
                if title_tag:
                    item["title"] = title_tag.get_text(strip=True)
                    item["link"] = urljoin(ALGUMON_URL, title_tag.get('href'))
                
                # ì œëª© ì—†ìœ¼ë©´ ìŠ¤í‚µ (ê´‘ê³  ë“±)
                if not item["title"]: continue

                # (2) ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
                # .icon-commenting-o ì•„ì´ì½˜ì„ ê°€ì§„ ë¶€ëª¨ ìš”ì†Œ(span)ë¥¼ ì°¾ìŒ
                comment_icon = post.select_one(".icon-commenting-o")
                if comment_icon:
                    # ì•„ì´ì½˜ ë°”ë¡œ ì˜†ì˜ ìˆ«ì í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    cmt_text = comment_icon.parent.get_text(strip=True)
                    # ìˆ«ìë§Œ ê±¸ëŸ¬ë‚´ê¸°
                    nums = re.findall(r'\d+', cmt_text)
                    if nums:
                        item["comments"] = int(nums[0])

                # (3) ë‚ ì§œ/ì‹œê°„ ì¶”ì¶œ
                # "22ë¶„ ì „" ê°™ì€ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” .created-at ë˜ëŠ” .deal-price-meta-info
                date_tag = post.select_one(".created-at")
                if not date_tag:
                    # ì—†ìœ¼ë©´ ë©”íƒ€ ì •ë³´ ì „ì²´ì—ì„œ ì‹œê°„ ì°¾ê¸°
                    meta_tag = post.select_one(".deal-price-meta-info")
                    if meta_tag:
                        item["date_text"] = meta_tag.get_text(strip=True)
                else:
                    item["date_text"] = date_tag.get_text(strip=True)

                all_items.append(item)

            except Exception as e:
                print(f"âš ï¸ íŒŒì‹± ì—ëŸ¬(ê°œë³„): {e}")
                continue

        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(all_items)}ê°œ ë°œê²¬")
        return json.dumps(all_items, ensure_ascii=False)

    except Exception as e:
        return json.dumps({"error": f"ì•Œêµ¬ëª¬ ì ‘ì† ì‹¤íŒ¨: {e}"}, ensure_ascii=False)

@mcp.tool()
def fetch_post_detail(url: str, content_selector: str) -> str:
    """ê²Œì‹œê¸€ ìƒì„¸(ëŒ“ê¸€) ìˆ˜ì§‘"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ì•Œêµ¬ëª¬ ëŒ“ê¸€ ì˜ì—­ (.post-content ë˜ëŠ” ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸)
        content = ""
        
        # ë³¸ë¬¸/ëŒ“ê¸€ í…ìŠ¤íŠ¸ ê¸ê¸°
        elements = soup.select(".post-content") # ê¸°ë³¸ ë³¸ë¬¸
        if not elements:
            # ëŒ“ê¸€ ì˜ì—­ì´ ë”°ë¡œ ìˆë‹¤ë©´ ì—¬ê¸° ì¶”ê°€ (ë³´í†µ ì•Œêµ¬ëª¬ì€ post-contentì— í¬í•¨ë¨)
            elements = soup.select(".comment-list")
            
        content = "\n".join([f"- {el.get_text(strip=True)[:200]}" for el in elements])
        
        if not content: return "ë‚´ìš©(ëŒ“ê¸€)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return content[:3000]
    except Exception as e:
        return f"ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"

if __name__ == "__main__":
    mcp.run()
